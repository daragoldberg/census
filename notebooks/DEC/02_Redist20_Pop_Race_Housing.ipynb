{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to compile data for geographies in the NYC Metro from Census redistricting files\n",
    "\n",
    "#### Requires local storage of Census zip files for each state as .pl format and place in folder directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from r_codes import geo_col,col1,col2,col3\n",
    "from geo import stco_fips,metro_codes,sub_7,sub_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pulling different variables, replace data columns in col_data\n",
    "col_head = {'o':geo_col,'1':col1,'2':col2,'3':col3}\n",
    "col_join = ['LOGRECNO','STUSAB','FILEID','CHARITER']\n",
    "col_data = ['STATE','COUNTY','GEOCODE','NAME','SUMLEV',\\\n",
    "            'P0010001','P0020002','P0020003','P0020005','P0020006',\\\n",
    "            'P0020007','P0020008','P0020009','P0020010','P0020011',\\\n",
    "            'P0040001','H0010001','H0010002','H0010003']\n",
    "sumlev = [40,50,60,160,140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_recode = {'GEOCODE':'id','NAME':'name','P0010001':'P_Tot','P0020002':'P_Hisp','P0020003':'P_NonHisp',\\\n",
    "              'P0020005':'P_White','P0020006':'P_Black','P0020007':'P_Other','P0020008':'P_Asian',\\\n",
    "              'P0020009':'P_Other','P0020010':'P_Other','P0020011':'P_Other','H0010001':'H_Tot',\\\n",
    "              'P0040001':'P_18p','H0010002':'H_Occ','H0010003':'H_Vac'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory\n",
    "folders = glob.glob('../data/red_20/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_recode = pd.read_csv('../data/geo/nyc_subbor_20.csv')\n",
    "\n",
    "#make adjusted subpl for calculation comparison over time\n",
    "subpl_recode = pd.read_csv('../data/geo/subpl20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data pulling functions for regional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state(state):\n",
    "    files = glob.glob(f'../data/red_20/{state}2020.pl/*.pl')\n",
    "    #for first file in folder\n",
    "    df = pd.read_table(f'{files[0]}',sep='|',header=None,low_memory=False)\n",
    "    df.columns = col_head[files[0][-8]]\n",
    "\n",
    "    #for all other files\n",
    "    for file in files[1:]:\n",
    "        dff = pd.read_table(f'{file}',sep='|',header=None,low_memory=False)\n",
    "        dff.columns = col_head[file[-8]]\n",
    "        df = pd.merge(df,dff,left_on=col_join,right_on=col_join,how=\"inner\")\n",
    "    \n",
    "    #reduce table size\n",
    "    df = df[col_data] #just the data columns we need\n",
    "    df = df[df.SUMLEV.isin(sumlev)].copy() #just the geo types we need\n",
    "    df = df[df.GEOCODE.isin(metro_codes)].copy() #just the places in our region\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(folders):\n",
    "    df = pd.DataFrame()\n",
    "    for folder in folders:\n",
    "        state = folder[15:17]\n",
    "        dff = make_state(state)\n",
    "        df = pd.concat([df,dff])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make initial table, clean up each geo level for master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master regional table with counties, munis, and NYC tracts\n",
    "df = make_table(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=col_recode)\n",
    "df = df.groupby(df.columns,axis=1).sum()\n",
    "df['STATE']= df['STATE'].astype(str).str.pad(width=2,side='left',fillchar='0')\n",
    "df['COUNTY']= df['COUNTY'].astype(int).astype(str).str.pad(width=3,side='left',fillchar='0')\n",
    "df['id']=df['id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make county table\n",
    "co = df[df['SUMLEV']==50].copy()\n",
    "co['stco'] = co.STATE + co.COUNTY\n",
    "co = co.drop(columns=['STATE','COUNTY','SUMLEV'])\n",
    "co['id']=co.id.astype(str).str.pad(width=5,side='left',fillchar='0')\n",
    "co['subreg'],co['geotype']=co['stco'].map(sub_7),'county'\n",
    "#county.to_csv('output/2020/pop_race_hou_co_reg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make subregion table\n",
    "sub = co.copy()\n",
    "sub = sub.drop(columns=['id','stco','name']).groupby(['subreg'],dropna=False).sum().reset_index()\n",
    "sub['name'],sub['id'],sub['geotype']=sub.subreg.map(sub_lbl),sub['subreg'],'subregion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make nyc subborough table\n",
    "nyc = df[df['SUMLEV']==140].copy()\n",
    "nyc = nyc.drop(columns=['STATE','COUNTY','name','SUMLEV'])\n",
    "nyc = pd.merge(nyc_recode,nyc,left_on='ct_id',right_on='id',how='left')\n",
    "nyc = nyc.drop(columns=['ct_id','ct_name','id']).groupby(['stco','id_sub','name']).sum().reset_index().rename(columns={'id_sub':'id'})\n",
    "nyc['geotype'],nyc['subreg']='municipality',nyc.stco.astype(str).map(sub_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make subplace table\n",
    "subpl = df[(df['SUMLEV']==160) | (df['SUMLEV']==60)]\n",
    "\n",
    "#export the 2020 geographies - not adjusted for longitudinal consistency\n",
    "subpl.to_csv('output/2020/pop_race_hou_subpl20.csv')\n",
    "\n",
    "#table for longitudinal analysis\n",
    "subpl = subpl.drop(columns=['STATE','COUNTY','SUMLEV','name'])\n",
    "subpl = pd.merge(subpl_recode,subpl,left_on='geoid20',right_on='id',how='left')\n",
    "subpl = subpl.drop(columns=['geoid20','name20','id_y',]).groupby(['stco','id_x','name']).sum().reset_index().rename(columns={'id_x':'id'})\n",
    "subpl['geotype'],subpl['subreg']='municipality',subpl.stco.astype(str).str.pad(width=5,side='left',fillchar='0').map(sub_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a national file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_files = glob.glob(f'../data/us2020.npl/*.pl')\n",
    "#for first file in folder\n",
    "us = pd.read_table(f'{us_files[0]}',sep='|',header=None,low_memory=False,encoding = 'unicode_escape')\n",
    "us.columns = col_head[us_files[0][-8]]\n",
    "\n",
    "#for all other files\n",
    "for file in us_files[1:]:\n",
    "    dff = pd.read_table(f'{file}',sep='|',header=None,low_memory=False,encoding = 'unicode_escape')\n",
    "    dff.columns = col_head[file[-8]]\n",
    "    us = pd.merge(us,dff,left_on=col_join,right_on=col_join,how=\"inner\")\n",
    "\n",
    "#reduce table size\n",
    "us = us[us.SUMLEV==10]\n",
    "us = us[col_data].rename(columns=col_recode).drop(columns=['STATE','COUNTY','SUMLEV'])\n",
    "us = us.groupby(us.columns,axis=1).sum()\n",
    "\n",
    "us['geotype']='nation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make master 2020 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "master20 = pd.concat([co,sub,nyc,subpl,us])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = ['id','name','stco','subreg']\n",
    "master20 = master20[id_col+[col for col in master20.columns if col not in id_col]]\n",
    "master20['yr']='2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "master20.to_csv('output/2020/pop_race_hou_allgeos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINE WITH 2000 & 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "master0010 = pd.read_csv('output/2000_2010/pop_race_hou_allgeos.csv').drop(columns='Unnamed: 0')\n",
    "#clean for table build\n",
    "master0010.stco = master0010['stco'].astype('Int64').astype(str).str.pad(width=5,side='left',fillchar='0')\n",
    "master0010.yr = master0010.yr.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.concat([master20,master0010])\n",
    "master.to_csv('output/pop_race_hou_allgeos.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
